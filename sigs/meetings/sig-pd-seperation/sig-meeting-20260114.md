本次会议为一场特殊的SIG例会，旨在合并讨论注意力机制压缩的相关议题。会议邀请了复旦大学纪涛老师分享其关于MHA to MLA实现大模型高效压缩部署的研究。

## 小结
**1. MHA to MLA 压缩范式**
- **背景与动机**: 随着模型规模增大，注意力机制（MHA）的计算量占比超过前馈网络（FFN），成为推理效率的主要瓶颈。MHA to MLA（多头潜在注意力）作为一种新架构，能显著减少KV缓存占用并提升性能，但其高昂的从头预训练成本使其难以直接应用于现有模型。
- **核心思想**: 提出一种“微创手术”范式，旨在将现有的GQA等架构模型，通过数学变换无缝迁移到MLA架构，从而复用现有模型的庞大参数和预训练知识。
- **两大“微创手术”**:
    - **Partial Rope (移除部分位置编码)**: 通过对模型各注意力头的tunom分数进行评估，识别并保留对模型性能贡献大的位置编码，移除其余冗余编码。后续研究优化了此策略，使其优先保留移除后会导致性能偏移大的频率。
    - **Joint Low-Rank Approximation (联合低秩近似)**: 将K和V的变换矩阵拼接后进行SVD分解并截断奇异值，得到一个低维的近似空间（ckv），从而大幅降低存储和计算成本。

**2. 多模态与长序列场景下的应用**
- **多模态模型**: 在处理图文等不同模态的输入时，发现联合进行低秩近似会导致性能劣化。因此，提出应为不同模态（如文本、图像）分别分配独立的低秩近似空间（如dkv=8 for text, dkv=8 for image），以获得更好的性能。
- **长序列推理**: 讨论了可学习的适配器（如RNN）作为另一种思路，通过区分短期记忆（局部窗口注意力）和长期记忆（RNN隐状态），来解决长序列推理时计算复杂度随长度线性增长的问题。

**3. 技术对比与挑战**
- **与其他技术对比**: 与量化技术（如2-bit量化）相比，MHA to MLA在压缩KV缓存的同时，能更好地控制模型性能损失。与DSA（稀疏注意力）结合使用，可进一步提升效率。
- **算子库挑战**: 当前的MLA算子库对输入维度（如query dim, pos_emb dim）有严格限制，这制约了压缩比例的灵活性。团队正在开发新的FlexMLA算子库以解决此问题。

## 录屏
https://www.bilibili.com/video/BV18yrsB9Ejy/ 