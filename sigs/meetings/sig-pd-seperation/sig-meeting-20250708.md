### 会议纪要

1. **项目介绍与开源计划**  
   - Ken Zhang 介绍了华为在推理加速领域的工作，并强调未来将秉持开放、透明、平等的原则推进社区发展。  
   - Omni-Infer定位为现有推理框架的加速补充，聚焦生成式硬件适配（如 NPU）、模型支持（如 DeepSeek-V3）及性能优化，目标实现与主流框架（如 vLLM）的无缝兼容。  
   - 当前版本（v0.1.0）支持大 EP（如 EP144）、前后处理优化（16K 并发 <3ms 延迟）、PT 分离部署等能力，性能达 400 QPS/50ms 时延。

2. **技术架构与能力**  
   - **分层设计**：底层为硬件适配层（NPU 亲和），中间层为加速套件（解耦优化能力），上层为企业级部署（非 Ominer 范畴）。  
   - **关键特性**：动态负载均衡、请求级调度、HADR 快速恢复、KV 传输（P2P 直连）、稠密模型支持（如千问 32B）。  
   - **PD 分离架构**：  
     - Global Proxy 控制请求拆分（Prefill → Decode）与负载均衡，支持动态调度策略（如 Least Load、Round Robin）。  
     - KV 传输通过 Database 实现，未来计划支持逐层传输以优化带宽利用率（A2/A3 硬件适配中）。  

3. **社区互动与问答**  
   - **组件独立性**：用户可选择性引入加速模块（如 Attention 优化），或仅对接底层 NPU 适配层。  
   - **性能对比**：未来将发布实测性能基准（如DeepSeek、千问等模型）。  
   - **A2 适配**：7 月推出初步版本，但需解决 KV 传输瓶颈（当前未支持分段拉取）。  
   - **扩展性规划**：  
     - 动态扩缩容（Prefill/Decode 实例增减、EP 调整、角色互换）。  
     - Global Proxy 多节点部署与高可用（需解决多机状态同步问题）。  

4. **后续计划**  
   - 开源社区将持续更新模型支持（如千问 MoE）、性能测试脚本及路线图。  
   - 鼓励用户提交需求（如私有化部署认证适配、Ray 资源策略集成）。  

### 会议录屏

会议录屏可在Omni-Infer的[B站查看](https://www.bilibili.com/video/BV1CruGzLEAa/?vd_source=2eac462af97360a47cf31d1fe4324da4)