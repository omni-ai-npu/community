### 会议纪要

1. **代码仓与目录结构**  
   - 当前所有代码已共享，`attention`相关技术存放在`omni/accelerator`目录下。  
   - 后续计划在`accelerator`下添加子目录，用于大模型推理加速的社区协作，重点关注`attention`加速领域。

2. **Attention机制分享**  
   - **背景**：大语言模型的`attention`矩阵具有稀疏性，部分`attention head`仅关注特定位置的token（如`think and recent tokens`）。  
   - **方法**：通过保留关键token（如首尾token）加速计算，数学定义为近似计算：`Q × K' × V' ≈ Q × K × V`，其中`K'`和`V'`为子集矩阵。  
   - **优化设计**：  
     - 以层为最小压缩单位（非`head`级），避免TP并行中的长尾效应。  
     - 基于遗传算法的搜索策略生成压缩模式（`pattern`），通过下游任务评分优化候选方案。  
   - **实现与性能**：  
     - 基于`vLLM 0.9.0`的`hybrid attention`功能，初始化两组KV缓存（压缩层与非压缩层）。  
     - 滑窗更新机制（如固定窗口大小为8）覆盖旧token。  
     - 当前仅在单节点开启，4P1D场景下提升`TPOT` 4毫秒，`QPM`达459。  

3. **未来计划**  
   - 探索更灵活的压缩方式（如动态索引选择、请求级压缩）。  
   - 开发`NPU`上的`block space attention`算子，支持算子内稀疏性。  
   - 解决压缩算法与`prefix`功能的冲突，结合`FlashAttention`优化。  

4. **问题讨论**  
   - 当前压缩模式仅与模型权重相关，未来可能扩展至请求级（如根据输入特征调整）。  

5. **社区协作**  
   - 鼓励更多开发者加入SIG，共同推进升腾平台上的`attention`加速技术。  
   - 每两周召开例会（周四晚7:00-7:30），通过Git仓库和微信群跟踪进展与讨论。  

### 会议录屏

可在[社区B站](https://www.bilibili.com/video/BV171bCzkEbN/)查看本次会议录屏

