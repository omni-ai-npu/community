本次会议围绕微软一篇关于基于强化学习的请求调度系统文章展开，探讨了其原理并针对性地讨论了在生产环境下应用时存在的gap。

## 小结
**1. 强化学习调度系统的介绍**
- 介绍了微软团队发表在EuroMLSyS上的文章，内容为基于RL的LLM调度系统。
- 该系统相较于传统的Round Robin调度方式，实现了约11%的端到端延迟降低，并能显著减小系统等待时间。
- 讨论了该研究中关于MDP（马尔科夫决策过程）的核心要素：
    - **状态(State)**：包括队列请求序列、Prompt长度（结合预先的length predictor）、各实例的执行时间和处理能力。
    - **动作(Action)**：将请求分配至指定的model instance，或选择不处理（hold）。
    - **奖励函数(Reward)**：由三个部分构成：排队惩罚（简化排队请求的处理）、完成奖励（接收到第I个请求的完成给予固定reward）及创新的启发式引导机制（初期引导模型遵循已知优策略，后期逐步放开）。

**2. 应用挑战与会议探讨**
- **可解释性与可控性**：参会者关注如何解释强化学习模型的学习行为，尤其是在生产环境中部署时，担忧其“黑盒”特性可能导致难以理解和控制，尤其是在面对异常或边缘情况时。
- **技术选型**：议题提到了on-policy与off-policy两种强化学习策略的适用场景。根据论文介绍，其所采用的是基于Q-learning的离线学习(off-policy)方法。

## 待办
**1. 内部实验进展同步**
- @@(莹莹)@@需要在后续会议或wg上分享其团队正在进行的强化学习策略实验相关的结果。

## 录屏

https://www.bilibili.com/video/BV19ZqYBGEys/ 